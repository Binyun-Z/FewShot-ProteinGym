{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "def return_position_single(mutation):\n",
    "    \"\"\"Note: Only works for single mutations\"\"\"\n",
    "    position = mutation.split(\":\")[0][1:-1]\n",
    "    return int(position)\n",
    "\n",
    "def keep_singles(DMS, mutant_column='mutant'):\n",
    "    DMS = DMS[~DMS[mutant_column].str.contains(\":\")]\n",
    "    return DMS\n",
    "\n",
    "\n",
    "def create_folds_random(DMS, n_folds=5, mutant_column='mutant'):\n",
    "    column_name = 'fold_random_{}'.format(n_folds)\n",
    "    try:\n",
    "        mutated_region_list = DMS[mutant_column].apply(lambda x: return_position_single(x)).unique()\n",
    "    except:\n",
    "        print(\"Mutated region not found from 'mutant' variable -- assuming the full protein sequence is mutated\")\n",
    "        mutated_region_list = range(len(DMS['mutated_sequence'].values[0]))\n",
    "    len_mutated_region = len(mutated_region_list)\n",
    "    if len_mutated_region < n_folds:\n",
    "        raise Exception(\"Error, there are fewer mutated regions than requested folds\")\n",
    "    DMS[column_name] = np.random.randint(0, n_folds, DMS.shape[0])\n",
    "    print(DMS[column_name].value_counts())\n",
    "    return DMS\n",
    "\n",
    "def create_folds_by_position_modulo(DMS, n_folds=5, mutant_column='mutant'):\n",
    "    column_name = 'fold_modulo_{}'.format(n_folds)\n",
    "    mutated_region_list = sorted(DMS[mutant_column].apply(return_position_single).unique())\n",
    "    len_mutated_region = len(mutated_region_list)\n",
    "    if len_mutated_region < n_folds:\n",
    "        raise Exception(\"Error, there are fewer mutated regions than requested folds\")\n",
    "    position_to_fold = {pos: i % n_folds for i, pos in enumerate(mutated_region_list)}\n",
    "    DMS[column_name] = DMS[mutant_column].apply(lambda x: position_to_fold[return_position_single(x)])\n",
    "    print(DMS[column_name].value_counts())\n",
    "    return DMS\n",
    "\n",
    "def create_folds_by_contiguous_position_discontiguous(DMS, n_folds=5, mutant_column='mutant'):\n",
    "    column_name = 'fold_contiguous_{}'.format(n_folds)\n",
    "    mutated_region_list = sorted(DMS[mutant_column].apply(lambda x: return_position_single(x)).unique())\n",
    "    len_mutated_region = len(mutated_region_list)\n",
    "    k, m = divmod(len_mutated_region, n_folds)\n",
    "    folds = [[i] * k + [i] * (i < m) for i in range(n_folds)]\n",
    "    folds = [item for sublist in folds for item in sublist]\n",
    "    folds_indices = dict(zip(mutated_region_list, folds))\n",
    "    if len_mutated_region < n_folds:\n",
    "        raise Exception(\"Error, there are fewer mutated regions than requested folds\")\n",
    "    DMS[column_name] = DMS[mutant_column].apply(lambda x: folds_indices[return_position_single(x)])\n",
    "    print(DMS[column_name].value_counts())\n",
    "    return DMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman(y_pred, y_true):\n",
    "    if np.var(y_pred) < 1e-6 or np.var(y_true) < 1e-6:\n",
    "        return 0.0\n",
    "    return spearmanr(y_pred, y_true)[0]\n",
    "\n",
    "def compute_stat(sr):\n",
    "    sr = np.asarray(sr)\n",
    "    mean = np.mean(sr)\n",
    "    std = np.std(sr)\n",
    "    sr = (sr,)\n",
    "    ci = list(bootstrap(sr, np.mean).confidence_interval)\n",
    "    return mean, std, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset_name, seed, shot, frac=0.2):\n",
    "    '''\n",
    "    sample the train data and test data\n",
    "    :param seed: sample seed\n",
    "    :param frac: the fraction of testing data, default to 0.2\n",
    "    :param shot: the size of training data\n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv(f'data/{dataset_name}/data.csv', index_col=0)\n",
    "    test_data = data.sample(frac=frac, random_state=seed)\n",
    "    train_data = data.drop(test_data.index)\n",
    "    kshot_data = train_data.sample(n=shot, random_state=seed)\n",
    "    assert len(kshot_data) == shot, (\n",
    "        f'expected {shot} train examples, received {len(train_data)}')\n",
    "\n",
    "    kshot_data.to_csv(f'data/{dataset_name}/train.csv')\n",
    "    test_data.to_csv(f'data/{dataset_name}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import stats\n",
    "from scipy.stats import bootstrap\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "class Mutation_Set(Dataset):\n",
    "    def __init__(self, data, tokenizer, sep_len=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = sep_len\n",
    "        self.seq, self.attention_mask = tokenizer(list(self.data['mutated_sequence']), padding='max_length',\n",
    "                                                  truncation=True,\n",
    "                                                  max_length=self.seq_len).values()\n",
    "\n",
    "        target = list(data['target_seq'])\n",
    "        self.target, self.tgt_mask = tokenizer(target, padding='max_length', truncation=True,\n",
    "                                               max_length=self.seq_len).values()\n",
    "        self.score = torch.tensor(np.array(self.data['DMS_score']))\n",
    "        self.pid = np.asarray(data['PID'])\n",
    "\n",
    "        if type(list(self.data['mut_pos'])[0]) != str:\n",
    "            self.position = [[u] for u in self.data['mut_pos']]\n",
    "\n",
    "        else:\n",
    "            self.position = []\n",
    "            for u in self.data['mut_pos']:\n",
    "                p = re.findall(r'\\d+', u)\n",
    "                pos = [int(v) for v in p]\n",
    "                self.position.append(pos)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.seq[idx], self.attention_mask[idx], self.target[idx],self.tgt_mask[idx] ,self.position[idx], self.score[idx], self.pid[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.score)\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        seq = torch.tensor(np.array([u[0] for u in data]))\n",
    "        att_mask = torch.tensor(np.array([u[1] for u in data]))\n",
    "        tgt = torch.tensor(np.array([u[2] for u in data]))\n",
    "        tgt_mask = torch.tensor(np.array([u[3] for u in data]))\n",
    "        pos = [torch.tensor(u[4]) for u in data]\n",
    "        score = torch.tensor(np.array([u[5] for u in data]), dtype=torch.float32)\n",
    "        pid = torch.tensor(np.array([u[6] for u in data]))\n",
    "        return seq, att_mask, tgt, tgt_mask, pos, score, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmForMaskedLM, EsmTokenizer, EsmConfig\n",
    "tokenizer = EsmTokenizer.from_pretrained(f'facebook/esm1v_t33_650M_UR90S_1')\n",
    "train_csv = pd.read_csv(f'../data/cleaned_split_data_singles/fold_random_5/data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Mutation_Set(data=train_csv, tokenizer=tokenizer)\n",
    "trainloader = DataLoader(trainset, batch_size=4, collate_fn=trainset.collate_fn, shuffle=True,num_workers=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 20, 15,  ..., 15,  8,  2],\n",
      "        [ 0, 20, 10,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 11,  ...,  1,  1,  1],\n",
      "        [ 0, 20, 18,  ...,  8,  5,  2]])\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(trainloader):\n",
    "    \n",
    "    seq, mask = data[0], data[1]\n",
    "    wt, wt_mask = data[2], data[3]\n",
    "    pos = data[4]  \n",
    "    mask_seq = seq.clone()\n",
    "    m_id = tokenizer.mask_token_id\n",
    "\n",
    "    batch_size = int(seq.shape[0])\n",
    "    for i in range(batch_size):\n",
    "        mut_pos = pos[i]\n",
    "        mask_seq[i, mut_pos+1] = m_id\n",
    "\n",
    "    print(mask_seq)\n",
    "    break\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SpecialTokensMixin.add_tokens of EsmTokenizer(name_or_path='facebook/esm1v_t33_650M_UR90S_1', vocab_size=33, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<cls>',\n",
       " '<pad>',\n",
       " '<eos>',\n",
       " '<unk>',\n",
       " 'L',\n",
       " 'A',\n",
       " 'G',\n",
       " 'V',\n",
       " 'S',\n",
       " 'E',\n",
       " 'R',\n",
       " 'T',\n",
       " 'I',\n",
       " 'D',\n",
       " 'P',\n",
       " 'K',\n",
       " 'Q',\n",
       " 'N',\n",
       " 'F',\n",
       " 'Y',\n",
       " 'M',\n",
       " 'H',\n",
       " 'W',\n",
       " 'C',\n",
       " 'X',\n",
       " 'B',\n",
       " 'U',\n",
       " 'Z',\n",
       " 'O',\n",
       " '.',\n",
       " '-',\n",
       " '<null_1>',\n",
       " '<mask>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerBase.encode() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m help(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerBase.encode() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
